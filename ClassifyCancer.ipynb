{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import unicodecsv as csv\n",
    "import tensorflow as tf\n",
    "#import PIL\n",
    "#from PIL import Image\n",
    "#print('Pillow Version:', PIL.__version__)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from resnets_utils import *\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"goal\": \"MAXIMIZE\",\n",
    "    \"maxTrials\":30,\n",
    "    \"maxParallelTrials\":2,\n",
    "    \"hyperparameterMetricTag\": \"accuracy\",\n",
    "    \"train_batch_size\":2,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"train_steps\":10000,\n",
    "    \"batch_norm\":\"True\",\n",
    "    \"ksize1\":5,\n",
    "    \"ksize2\":5,\n",
    "    \"nfil1\":10,\n",
    "    \"nfil2\":20,\n",
    "    \"dprob\":0.25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAXIMIZE\n",
      "30\n",
      "2\n",
      "accuracy\n",
      "2\n",
      "0.001\n",
      "10000\n",
      "True\n",
      "5\n",
      "5\n",
      "10\n",
      "20\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(hparams.get(\"goal\", 5))\n",
    "print(hparams.get(\"maxTrials\", 5))\n",
    "print(hparams.get(\"maxParallelTrials\", 5))\n",
    "print(hparams.get(\"hyperparameterMetricTag\", 5))\n",
    "print(hparams.get(\"train_batch_size\", 2))\n",
    "print(hparams.get(\"learning_rate\", 5))\n",
    "print(hparams.get(\"train_steps\", 5))\n",
    "print(hparams.get(\"batch_norm\", 5))\n",
    "print(hparams.get(\"ksize1\", 5))\n",
    "print(hparams.get(\"ksize2\", 5))\n",
    "print(hparams.get(\"nfil1\", 5))\n",
    "print(hparams.get(\"nfil2\", 5))\n",
    "print(hparams.get(\"dprob\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\Kaigenicsproj001\\etrain\n",
      ".\\kaigenicsproj001\\eval\n"
     ]
    }
   ],
   "source": [
    "traindirpath = \".\\Kaigenicsproj001\\etrain\"\n",
    "evaldirpath = \".\\kaigenicsproj001\\eval\"\n",
    "outputdirpath = \".\\outputdir\"\n",
    "trainfilecount = 0\n",
    "trainfilenames = []\n",
    "evalfilecount = 0\n",
    "evalfilenames = []\n",
    "trainmaxpoints = []\n",
    "trainmaxpoint = 0\n",
    "evalmaxpoints = []\n",
    "evalmaxpoint = 0\n",
    "trainimagelist = []\n",
    "trainlabellist = []\n",
    "trainborderpointslist = []\n",
    "evalimagelist = []\n",
    "evallabellist = []\n",
    "evalborderpointslist = []\n",
    "print(traindirpath)\n",
    "print(evaldirpath)\n",
    "\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "NCLASSES = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_matfile(index,filename):\n",
    "    \"\"\"\n",
    "    this will take a file and extract the image\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = []\n",
    "    with h5py.File(traindirpath +\"\\\\\" + filename, 'r') as f: # open file\n",
    "        f.visit(keys.append) # append all keys to list\n",
    "        print(filename)\n",
    "        #print(keys)\n",
    "        #Y1 = f.get('cjdata/PID') \n",
    "        Y2 = f.get('cjdata/image') \n",
    "        Y3 = f.get('cjdata/label')\n",
    "        Y4 = f.get('cjdata/tumorBorder')\n",
    "\n",
    "        #Y5 = f.get('cjdata/tumorMask')\n",
    "        #X1 = np.array(Y1)\n",
    "        X2 = np.array(Y2)\n",
    "        X3 = np.array(Y3)\n",
    "        #print(filename)\n",
    "        #print(X3)\n",
    "        X4 = np.array(Y4)\n",
    "\n",
    "        #X5 = np.array(Y5)\n",
    "        #print(X1)\n",
    "        #print(X2) \n",
    "        #print(X3)\n",
    "        #numberofpoints= Y4.shape[1]\n",
    "        trainmaxpoints.append(X4.shape[1])\n",
    "        trainimagelist.append(X2)\n",
    "        trainlabellist.append(X3)\n",
    "        trainborderpointslist.append(X4)\n",
    "        \n",
    "    return 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eval_matfile(index,filename):\n",
    "    \"\"\"\n",
    "    this will take a file and extract the image\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = []\n",
    "    with h5py.File(evaldirpath +\"\\\\\" + filename, 'r') as f: # open file\n",
    "        f.visit(keys.append) # append all keys to list\n",
    "        print(filename)\n",
    "        #print(keys)\n",
    "        #Y1 = f.get('cjdata/PID') \n",
    "        Y2 = f.get('cjdata/image') \n",
    "        Y3 = f.get('cjdata/label')\n",
    "        Y4 = f.get('cjdata/tumorBorder')\n",
    "\n",
    "        #Y5 = f.get('cjdata/tumorMask')\n",
    "        #X1 = np.array(Y1)\n",
    "        X2 = np.array(Y2)\n",
    "        X3 = np.array(Y3)\n",
    "        #print(filename)\n",
    "        #print(X3)\n",
    "        X4 = np.array(Y4)\n",
    "\n",
    "        #X5 = np.array(Y5)\n",
    "        #print(X1)\n",
    "        #print(X2) \n",
    "        #print(X3)\n",
    "        #numberofpoints= Y4.shape[1]\n",
    "        evalmaxpoints.append(X4.shape[1])\n",
    "        evalimagelist.append(X2)\n",
    "        evallabellist.append(X3)\n",
    "        evalborderpointslist.append(X4)\n",
    "        \n",
    "    return 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\Kaigenicsproj001\\etrain\n",
      "1301.mat\n",
      "1302.mat\n",
      "1911.mat\n",
      "1912.mat\n",
      "1913.mat\n",
      "1914.mat\n",
      "1915.mat\n",
      "1916.mat\n",
      "2.mat\n",
      "3.mat\n",
      "4.mat\n",
      "731.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:11: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 28, 22, 44, 46, 26, 36, 38, 64, 58, 62, 134]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(traindirpath)\n",
    "for root, dirs, files in os.walk(traindirpath):\n",
    "    for filename in files:\n",
    "            trainfilecount+=1\n",
    "            trainfilenames.append(filename)\n",
    "            \n",
    "for i in range(trainfilecount):\n",
    "    process_train_matfile(i,trainfilenames[i])\n",
    "\n",
    "%matplotlib inline\n",
    "plot.hist(trainmaxpoints, normed=True, bins=30)\n",
    "plot.ylabel('maxpoints');\n",
    "print(trainmaxpoints)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.mat\n",
      "1303.mat\n",
      "1917.mat\n",
      "731.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:10: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 38, 60, 134]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(evaldirpath):\n",
    "    for filename in files:\n",
    "            evalfilecount+=1\n",
    "            evalfilenames.append(filename)\n",
    "            \n",
    "for i in range(evalfilecount):\n",
    "    process_eval_matfile(i,evalfilenames[i])\n",
    "\n",
    "%matplotlib inline\n",
    "plot.hist(evalmaxpoints, normed=True, bins=30)\n",
    "plot.ylabel('maxpoints');\n",
    "print(evalmaxpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(12, 512, 512)\n",
      "(12, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trainimagelist[0].shape)\n",
    "trainfullimagearray = np.array(trainimagelist)\n",
    "print(trainfullimagearray.shape)\n",
    "reshapedtrainimagearray = trainfullimagearray.reshape(trainfilecount,512,512,1)\n",
    "#print(reshapedtrainarray)\n",
    "print(reshapedtrainimagearray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(4, 512, 512)\n",
      "(4, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(evalimagelist[0].shape)\n",
    "evalfullimagearray = np.array(evalimagelist)\n",
    "print(evalfullimagearray.shape)\n",
    "reshapedevalimagearray = evalfullimagearray.reshape(evalfilecount,512,512,1)\n",
    "#print(reshapedtrainarray)\n",
    "print(reshapedevalimagearray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(12, 1, 1)\n",
      "[3. 3. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.]\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "print(trainlabellist[0].shape)\n",
    "trainfulllabelarray = np.array(trainlabellist)\n",
    "print(trainfulllabelarray.shape)\n",
    "reshapedtrainlabelarray = trainfulllabelarray.reshape(trainfilecount)\n",
    "print(reshapedtrainlabelarray)\n",
    "print(reshapedtrainlabelarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(12, 1, 1)\n",
      "[1. 3. 2. 2.]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(evallabellist[0].shape)\n",
    "evalfulllabelarray = np.array(evallabellist)\n",
    "print(trainfulllabelarray.shape)\n",
    "reshapedevallabelarray = evalfulllabelarray.reshape(evalfilecount)\n",
    "print(reshapedevallabelarray)\n",
    "print(reshapedevallabelarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.]\n",
      "12\n",
      "[3 3 2 2 2 2 2 2 1 1 1 2]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(reshapedtrainlabelarray.ravel())\n",
    "#one hot train set label\n",
    "num_labels = reshapedtrainlabelarray.shape[0]\n",
    "print(num_labels)\n",
    "reshapedtrainlabelarrayintegerlist = reshapedtrainlabelarray.astype(int)\n",
    "reshapedtrainlabelarrayinteger = np.array(reshapedtrainlabelarrayintegerlist)\n",
    "print(reshapedtrainlabelarrayinteger)\n",
    "trainlabelsonehot = np.eye(NCLASSES)[reshapedtrainlabelarrayinteger-1]\n",
    "print(trainlabelsonehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 3. 2. 2.]\n",
      "4\n",
      "[1 3 2 2]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(reshapedevallabelarray.ravel())\n",
    "#one hot train set label\n",
    "num_labels = reshapedevallabelarray.shape[0]\n",
    "print(num_labels)\n",
    "reshapedevallabelarrayintegerlist = reshapedevallabelarray.astype(int) \n",
    "reshapedevallabelarrayinteger = np.array(reshapedevallabelarrayintegerlist)\n",
    "print(reshapedevallabelarrayinteger)\n",
    "evallabelsonehot = np.eye(NCLASSES)[reshapedevallabelarrayinteger-1]\n",
    "print(evallabelsonehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "# load image as pixel array\n",
    "data = trainimagelist[0]\n",
    "# summarize shape of the pixel array\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "# display the array of pixels as an image\n",
    "pyplot.imshow(data)\n",
    "pyplot.show()\n",
    "\"\"\"\n",
    "        "
   ]
  },
  

{
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    print(\"in identity block\")\n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    print(X.shape)\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "    print(X.shape)\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    print(X.shape)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.compat.v1.reset_default_graph() \n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = resnet_identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  
{
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    print(\"in conv block\")\n",
    "    print(X.shape)\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    \n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '2a', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    \n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
    "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = resnet_convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (512, 512, 1), classes = 3):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    print(X_input.shape)\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    print(\"shape1:\")\n",
    "    print(X.shape)\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    print(\"shape2:\")\n",
    "    print(X.shape)\n",
    "    # Stage 2\n",
    "    X = resnet_convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = resnet_identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = resnet_identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "    \n",
    "    print(\"shape3:\")\n",
    "    print(X.shape)\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    \n",
    "    X = resnet_convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = resnet_identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = resnet_identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = resnet_identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "    \n",
    "    print(\"shape4:\")\n",
    "    print(X.shape)\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = resnet_convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = resnet_identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = resnet_identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = resnet_identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = resnet_identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = resnet_identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    \n",
    "    print(\"shape5:\")\n",
    "    print(X.shape)\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = resnet_convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = resnet_identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = resnet_identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    print(\"shape6:\")\n",
    "    print(X.shape)\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D((2, 2))(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    print(\"shape7:\")\n",
    "    print(X.shape)\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    print(\"shape8:\")\n",
    "    print(X.shape)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNetFifty')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 512, 1)\n",
      "shape1:\n",
      "(None, 518, 518, 1)\n",
      "shape2:\n",
      "(None, 127, 127, 64)\n",
      "in conv block\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 256)\n",
      "(None, 127, 127, 256)\n",
      "in identity block\n",
      "(None, 127, 127, 256)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 256)\n",
      "(None, 127, 127, 256)\n",
      "in identity block\n",
      "(None, 127, 127, 256)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 64)\n",
      "(None, 127, 127, 256)\n",
      "(None, 127, 127, 256)\n",
      "shape3:\n",
      "(None, 127, 127, 256)\n",
      "in conv block\n",
      "(None, 127, 127, 256)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 512)\n",
      "in identity block\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 512)\n",
      "in identity block\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 512)\n",
      "in identity block\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 128)\n",
      "(None, 64, 64, 512)\n",
      "(None, 64, 64, 512)\n",
      "shape4:\n",
      "(None, 64, 64, 512)\n",
      "in conv block\n",
      "(None, 64, 64, 512)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "in identity block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "in identity block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "in identity block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "in identity block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "in identity block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 32, 32, 1024)\n",
      "shape5:\n",
      "(None, 32, 32, 1024)\n",
      "in conv block\n",
      "(None, 32, 32, 1024)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 2048)\n",
      "(None, 16, 16, 2048)\n",
      "in identity block\n",
      "(None, 16, 16, 2048)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 2048)\n",
      "(None, 16, 16, 2048)\n",
      "in identity block\n",
      "(None, 16, 16, 2048)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 512)\n",
      "(None, 16, 16, 2048)\n",
      "(None, 16, 16, 2048)\n",
      "shape6:\n",
      "(None, 16, 16, 2048)\n",
      "shape7:\n",
      "(None, 8, 8, 2048)\n",
      "shape8:\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape = (512, 512, 1), classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 12\n",
      "number of test examples = 4\n",
      "X_train shape: (12, 512, 512, 1)\n",
      "Y_train shape: (12, 3)\n",
      "X_test shape: (4, 512, 512, 1)\n",
      "Y_test shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "reshapedtrainimagearray, reshapedevalimagearray, trainlabelsonehot, evallabelsonehot\n",
    "# Normalize image vectors\n",
    "X_train = reshapedtrainimagearray/255.\n",
    "X_test = reshapedevalimagearray/255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = trainlabelsonehot #convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = evallabelsonehot #convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12 samples\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - 267s 22s/sample - loss: 59.2798 - accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 185s 15s/sample - loss: 18.6736 - accuracy: 0.4167\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 183s 15s/sample - loss: 21.5662 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f9e8727f88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 3, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/1 [========================================================================================================================] - 117s 29s/sample - loss: 243054.7812 - accuracy: 0.5000\n",
      "Loss = 243054.78125\n",
      "Test Accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(img, mode, hparams):\n",
    "    ksize1 = hparams.get(\"ksize1\", 5)\n",
    "    ksize2 = hparams.get(\"ksize2\", 5)\n",
    "    nfil1 = hparams.get(\"nfil1\", 10)\n",
    "    nfil2 = hparams.get(\"nfil2\", 20)\n",
    "    dprob = hparams.get(\"dprob\", 0.25)\n",
    "\n",
    "    c0 = tf.layers.conv2d(inputs = img, filters = nfil1,\n",
    "                          kernel_size = ksize1, strides = 1,\n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT, WIDTH, nfil1)\n",
    "    \n",
    "    p0 = tf.layers.max_pooling2d(inputs = c0, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil1)\n",
    "    \n",
    "    c1 = tf.layers.conv2d(inputs = p0, filters = nfil1,\n",
    "                          kernel_size = ksize1, strides = 1,\n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT, WIDTH, nfil1)\n",
    "    \n",
    "    p1 = tf.layers.max_pooling2d(inputs = c1, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil1)\n",
    "    \n",
    "    c2 = tf.layers.conv2d(inputs = p1, filters = nfil2,\n",
    "                          kernel_size = ksize2, strides = 1, \n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil2)\n",
    "    \n",
    "    p2 = tf.layers.max_pooling2d(inputs = c2, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 4, WIDTH // 4, nfil2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2] * p2.shape[3] # HEIGHT // 4 * WIDTH // 4 * nfil2\n",
    "    p2flat = tf.reshape(tensor = p2, shape = [-1, outlen]) # shape = (batch_size, HEIGHT // 4 * WIDTH // 4 * nfil2)\n",
    "\n",
    "    # Apply batch normalization\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 400, activation = None)\n",
    "        h3 = tf.layers.batch_normalization(inputs = h3, training = (mode == tf.estimator.ModeKeys.TRAIN)) # only batchnorm when training\n",
    "        h3 = tf.nn.relu(features = h3)\n",
    "    else:  \n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 400, activation = tf.nn.relu)\n",
    "  \n",
    "    # Apply dropout\n",
    "    h3d = tf.layers.dropout(inputs = h3, rate = dprob, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    ylogits = tf.layers.dense(inputs = h3d, units = NCLASSES, activation = None)\n",
    "  \n",
    "    # Apply batch normalization once more\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        ylogits = tf.layers.batch_normalization(inputs = ylogits, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    return ylogits, NCLASSES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    # Input will be rank 3\n",
    "    feature_placeholders = {\"image\": tf.placeholder(dtype = tf.float32, shape = [None, HEIGHT, WIDTH])}\n",
    "    # But model function requires rank 4\n",
    "    features = {\"image\": tf.expand_dims(input = feature_placeholders[\"image\"], axis = -1)} \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_classifier(features, labels, mode, params):\n",
    "        \n",
    "    ylogits, nclasses = cnn_model(features[\"image\"], mode, params)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits = ylogits)\n",
    "    class_ids = tf.cast(x = tf.argmax(input = probabilities, axis = 1), dtype = tf.uint8)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.reduce_mean(input_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(logits = ylogits, labels = labels))\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss, \n",
    "                    global_step = tf.train.get_global_step(),\n",
    "                    learning_rate = params[\"learning_rate\"], \n",
    "                    optimizer = \"Adam\")\n",
    "            eval_metric_ops = None\n",
    "        else:\n",
    "            train_op = None\n",
    "            eval_metric_ops =  {\"accuracy\": tf.metrics.accuracy(labels = tf.argmax(input = labels, axis = 1), predictions = class_ids)}\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    " \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = {\"probabilities\": probabilities, \"class_ids\": class_ids},\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = {\"predictions\":tf.estimator.export.PredictOutput({\"probabilities\": probabilities, \"class_ids\": class_ids})}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir, hparams):\n",
    "    #tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    EVAL_INTERVAL = 60\n",
    "    \n",
    "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x = {\"image\": reshapedtrainimagearray},\n",
    "        y = trainlabelsonehot,\n",
    "        batch_size = 2,\n",
    "        num_epochs = None,\n",
    "        shuffle = True,\n",
    "        queue_capacity = 5000\n",
    "    )\n",
    "\n",
    "    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x = {\"image\": reshapedevalimagearray},\n",
    "        y = evallabelsonehot,\n",
    "        batch_size = 2,\n",
    "        num_epochs = 1,\n",
    "        shuffle = False,\n",
    "        queue_capacity = 5000\n",
    "    )\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = image_classifier,\n",
    "        model_dir = output_dir,\n",
    "        params = hparams)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps = hparams[\"train_steps\"])\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = eval_input_fn,\n",
    "        steps = None,\n",
    "        exporters = exporter)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_and_evaluate(outputdirpath, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
